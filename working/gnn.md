```python
from google.colab import drive
import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler

pd.set_option("display.expand_frame_repr", False)
pd.set_option("display.max_columns", None)
pd.set_option("display.min_rows", 50)
pd.set_option("display.max_rows", 50)
pd.set_option("display.width", None)

drive_path = "/content/drive"
drive.mount(drive_path)
data_path = f"{drive_path}/My Drive/Colab Notebooks/gnn/input/march-machine-learning-mania-2025"
```

    Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).



```python
res = pd.DataFrame()

for type_ in ["RegularSeason", "NCAATourney"]:
  res = pd.concat([res, pd.read_csv(f"{data_path}/M{type_}DetailedResults.csv")])

res = res.reset_index(drop=True)

res.insert(0, "WID", res["Season"].astype(str) + "_" + res["DayNum"].astype(str).str.zfill(3) + "_" + res["WTeamID"].astype(str) + "_" + res["LTeamID"].astype(str))
res.loc[res["WTeamID"] < res["LTeamID"], "WID"] = res["Season"].astype(str) + "_" + res["DayNum"].astype(str).str.zfill(3) + "_" + res["LTeamID"].astype(str) + "_" + res["WTeamID"].astype(str)
res.insert(1, "LID", res["WID"])

res.insert(2, "WSeason", res.pop("Season"))
res.insert(3, "LSeason", res["WSeason"])

res.insert(4, "WDayNum", res.pop("DayNum"))
res.insert(5, "LDayNum", res["WDayNum"])

res.insert(7, "LTeamID", res.pop("LTeamID"))

res.insert(8, "WOppID", res["LTeamID"])
res.insert(9, "LOppID", res["WTeamID"])

res.insert(10, "WMargin", res["WScore"] - res["LScore"])
res.insert(11, "LMargin", -res["WMargin"])

res.insert(12, "WLoc", res.pop("WLoc").map({"A": -1, "N": 0, "H": 1}))
res.insert(13, "LLoc", -res["WLoc"])

res.insert(14, "WNumOT", res.pop("NumOT"))
res.insert(15, "LNumOT", res["WNumOT"])

res = pd.concat([
    res[[c for c in res if c[0] == "W"]].rename(columns={c: c[1:] for c in res if c[0] == "W"}),
    res[[c for c in res if c[0] == "L"]].rename(columns={c: c[1:] for c in res if c[0] == "L"}),
]).sort_values("ID").reset_index(drop=True)

for c in res.loc[:, "Season":"OppID"]:
  res[c] = res[c].astype("int32")

sy = StandardScaler()
cy = "Margin"
res[cy] = sy.fit_transform(res[[cy]]).astype("float32")

cf = "Loc"
for c in res.loc[:, cf:]:
  res[c] = res[c].astype("float32")

res.loc[:, cf:] = StandardScaler().fit_transform(res.loc[:, cf:])

print(f"res {res.shape}")
print(res)
print()
res.info()
```

    res (240528, 22)
                            ID  Season  DayNum  TeamID  OppID    Margin       Loc     NumOT     Score       FGM       FGA      FGM3      FGA3       FTM       FTA        OR        DR       Ast        TO       Stl       Blk        PF
    0       2003_010_1328_1104    2003      10    1104   1328  0.397053  0.000000 -0.225257 -0.153137  0.490802  0.226886 -1.252690 -0.951573 -0.476284 -0.233940  0.862797  0.073893 -0.012935  2.304793  0.173707 -1.023865  0.846855
    1       2003_010_1328_1104    2003      10    1328   1104 -0.397053  0.000000 -0.225257 -0.634959 -0.537384 -0.438796 -1.583592 -1.611310  0.346381  0.273258 -0.095526 -0.317623 -1.148182  1.129768  0.848504 -0.583745  0.399831
    2       2003_010_1393_1272    2003      10    1393   1272 -0.463229  0.000000 -0.225257 -0.554655 -0.126110  1.425113 -0.259983  0.697769 -0.805351  0.019659  2.300281  0.269650 -1.375231 -0.280261  0.511106  1.176735 -0.494217
    3       2003_010_1393_1272    2003      10    1272   1393  0.463229  0.000000 -0.225257  0.007471  0.285165  0.759431  0.401821  0.038032 -0.640818 -0.107141  1.102378  0.856923  0.668213 -0.045256 -0.838489  0.296495 -0.047193
    4       2003_011_1400_1208    2003      11    1208   1400 -0.397053  0.000000 -0.225257  0.087774 -0.126110  0.759431 -0.259983 -0.621705  0.510914  0.907256  2.539862 -1.687925 -0.239984 -0.750271  0.173707 -1.023865 -0.941241
    5       2003_011_1400_1208    2003      11    1400   1208  0.397053  0.000000 -0.225257  0.569596  1.107713  0.626295 -0.259983 -0.951573 -0.476284 -0.867938  1.581539 -0.317623 -0.239984  0.189749 -0.838489  0.296495  0.399831
    6       2003_011_1437_1266    2003      11    1266   1437  0.794106  0.000000 -0.225257  0.248382 -0.126110  0.226886  0.401821 -0.291836  0.510914  1.160855  1.581539  0.465408  0.441164 -0.750271 -0.501090 -0.583745  1.517391
    7       2003_011_1437_1266    2003      11    1437   1266 -0.794106  0.000000 -0.225257 -0.715262 -0.537384  2.223930 -1.252690  1.027637  0.017315  0.400057  4.935669 -0.317623 -0.921132 -0.280261 -1.513286  0.736615  1.070367
    8       2003_011_1457_1296    2003      11    1457   1296 -0.397053  0.000000 -0.225257 -1.598603 -1.359932 -0.971341 -0.259983  0.367900 -0.969884 -0.614339  1.581539 -0.709137 -0.921132  1.364774 -0.838489 -0.143625  1.070367
    9       2003_011_1457_1296    2003      11    1296   1457  0.397053  0.000000 -0.225257 -1.116781 -1.359932 -2.435840 -1.252690 -1.776244  0.510914  1.414454 -1.053849 -0.904895 -0.467034 -0.280261  2.535497 -0.583745 -0.047193
    10      2003_011_1458_1186    2003      11    1458   1186  1.720564  1.062083 -0.225257  0.890811  0.285165  0.093750 -0.259983 -1.281442  1.498112  0.907256  0.383635  0.073893 -0.239984 -0.985276  0.848504 -0.143625 -0.047193
    11      2003_011_1458_1186    2003      11    1186   1458 -1.720564 -1.062083 -0.225257 -1.197084 -0.948658 -1.370750 -1.252690 -1.446376 -0.311751 -0.360740 -1.053849 -0.317623 -1.148182  1.364774 -0.838489 -0.143625  1.517391
    12      2003_012_1194_1156    2003      12    1194   1156  0.330878  0.000000 -0.225257  0.087774  0.696439  0.226886 -0.590885 -1.446376 -0.640818 -0.233940 -0.335107 -0.317623 -0.921132  0.894764  0.848504 -0.583745  1.070367
    13      2003_012_1194_1156    2003      12    1156   1194 -0.330878  0.000000 -0.225257 -0.313744 -0.126110 -0.571932 -0.259983 -0.291836 -0.311751  0.907256  0.623216  0.465408 -0.012935  2.774803  0.511106 -0.583745 -0.047193
    14      2003_012_1236_1161    2003      12    1161   1236  1.191160  1.062083 -0.225257  0.810507 -0.331747 -0.172523 -1.583592 -1.941178  2.978909  2.428851  0.623216 -1.100653  0.214114  0.894764  1.523301 -1.023865  1.517391
    15      2003_012_1236_1161    2003      12    1236   1161 -1.191160 -1.062083 -0.225257 -0.634959 -1.154295 -2.036431 -0.921788 -0.786639  1.004513  1.034055 -0.335107 -0.513380 -0.467034  3.949829  1.185903  0.296495  2.187927
    16      2003_012_1457_1186    2003      12    1186   1457  0.926457  0.000000 -0.225257  0.408989  0.696439  0.759431 -0.921788 -0.951573  0.181848  0.146458  0.623216  2.227225  1.349361  1.364774  0.173707 -0.583745  0.623343
    17      2003_012_1457_1186    2003      12    1457   1186 -0.926457  0.000000 -0.225257 -0.715262 -0.948658  0.360022 -0.921788 -0.456771  0.510914  0.400057 -0.574688  0.269650 -0.694083  0.424754  2.535497  2.056974 -0.047193
    18      2003_012_1458_1296    2003      12    1458   1296  1.852915  1.062083 -0.225257  1.131722  1.518987  1.425113 -0.590885 -0.456771  0.181848 -0.107141  0.862797 -0.317623 -0.467034 -1.690291  1.860700 -1.463985 -1.164753
    19      2003_012_1458_1296    2003      12    1296   1458 -1.852915 -1.062083 -0.225257 -1.116781 -0.331747 -0.571932 -1.252690 -0.951573 -1.134417 -0.994738 -0.335107 -0.121865 -0.694083  1.129768 -1.850684 -0.143625 -0.047193
    20      2003_013_1202_1106    2003      13    1106   1202 -0.066176  0.000000 -0.225257  0.248382  0.902076  0.892568  1.063626  0.367900 -1.463483 -1.882335  0.623216 -1.492168  0.441164 -0.280261 -0.163691 -0.583745 -1.388265
    21      2003_013_1202_1106    2003      13    1202   1106  0.066176  0.000000 -0.225257  0.328685  0.902076 -0.705068  0.070919 -1.116507 -0.805351 -1.121537 -1.053849 -0.513380  1.122311  0.424754  0.173707 -1.023865 -2.952849
    22      2003_013_1237_1135    2003      13    1135   1237 -0.066176  0.000000 -0.225257 -0.394048 -0.126110 -0.039387 -0.259983 -0.126902 -0.476284 -0.360740  0.862797 -0.513380  0.895262  1.129768  0.511106  0.296495 -1.164753
    23      2003_013_1237_1135    2003      13    1237   1135  0.066176  0.000000 -0.225257 -0.313744  0.285165  1.291976 -0.590885 -0.126902 -0.805351 -0.867938  2.539862 -0.121865  0.441164  0.894764  1.860700 -0.143625 -0.270705
    24      2003_013_1323_1125    2003      13    1125   1323 -1.852915 -1.062083 -0.225257 -1.759210 -1.359932  1.025704  0.401821  0.697769 -1.628016 -1.501936  0.862797  0.465408 -0.239984  0.894764  1.185903 -1.463985 -0.270705
    ...                    ...     ...     ...     ...    ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...
    240503  2025_131_1397_1120    2025     131    1397   1120  0.330878  0.000000 -0.225257  0.007471 -0.948658 -1.237613 -0.590885 -0.126902  1.827178  0.907256 -0.095526 -0.317623 -1.375231 -0.750271 -0.501090 -0.583745 -0.047193
    240504  2025_131_1408_1272    2025     131    1408   1272 -0.066176  0.000000 -0.225257  0.569596  0.490802  0.493159  0.732723  1.522440  0.017315  0.146458 -1.293430  0.269650  0.668213 -0.280261  0.848504  0.296495  1.964415
    240505  2025_131_1408_1272    2025     131    1272   1408  0.066176  0.000000 -0.225257  0.649900  0.079527  1.158840 -0.590885  0.532834  1.498112  2.302051  2.300281  0.073893  0.668213 -0.045256  0.848504  0.736615 -0.270705
    240506  2025_131_1412_1317    2025     131    1317   1412 -0.661755  0.000000 -0.225257 -1.116781 -0.948658  0.759431 -0.921788 -0.291836 -0.311751 -0.487540  1.581539 -0.709137 -0.921132 -0.985276 -0.163691 -0.583745 -0.941241
    240507  2025_131_1412_1317    2025     131    1412   1317  0.661755  0.000000 -0.225257 -0.313744 -0.126110 -0.039387 -0.921788 -0.621705  0.017315 -0.107141  0.623216 -0.513380 -0.012935 -1.220281 -0.838489  0.736615 -0.941241
    240508  2025_131_1430_1213    2025     131    1430   1213 -0.463229  1.062083 -0.225257  0.971115  0.490802  0.226886  0.732723 -0.126902  0.839980  1.034055  0.144054 -1.100653 -0.012935 -0.515266 -1.175887 -1.023865  1.517391
    240509  2025_131_1430_1213    2025     131    1213   1430  0.463229 -1.062083 -0.225257  1.533240  0.696439 -0.305659 -0.259983 -0.456771  2.156244  1.668053 -1.053849 -0.709137 -0.694083 -1.220281  0.173707  1.616855  0.846855
    240510  2025_131_1433_1260    2025     131    1260   1433 -0.463229  0.000000 -0.225257 -1.197084 -0.948658  1.558249 -0.590885  0.862703 -0.640818 -0.487540  1.821120 -0.121865 -0.694083 -0.750271  0.173707  0.296495 -0.047193
    240511  2025_131_1433_1260    2025     131    1433   1260  0.463229  0.000000 -0.225257 -0.634959 -0.537384 -0.705068 -0.259983  0.697769 -0.311751  0.146458 -0.574688  0.856923 -0.467034 -0.750271 -0.163691  1.176735 -0.494217
    240512  2025_131_1458_1277    2025     131    1458   1277  0.198527  0.000000 -0.225257  0.569596 -0.331747 -0.305659  0.732723  1.522440  1.333579  1.034055 -1.293430  0.661165  0.441164 -1.455286 -1.175887  0.296495  0.623343
    240513  2025_131_1458_1277    2025     131    1277   1458 -0.198527  0.000000 -0.225257  0.328685 -0.126110  0.360022  0.070919 -0.126902  0.839980  0.526857 -0.574688  0.856923 -0.012935 -1.455286 -1.850684 -0.583745  0.399831
    240514  2025_131_1463_1343    2025     131    1463   1343  0.132351  0.000000 -0.225257 -0.875870 -0.537384 -0.971341 -1.252690 -0.786639 -0.311751 -0.233940 -0.814269  0.465408 -1.375231 -1.220281 -1.175887 -0.143625 -2.058801
    240515  2025_131_1463_1343    2025     131    1343   1463 -0.132351  0.000000 -0.225257 -1.036477 -0.537384  0.093750  1.063626  2.512045 -1.792549 -2.135934 -1.293430 -0.709137  0.214114 -1.455286 -0.501090 -0.583745 -0.270705
    240516  2025_131_1471_1414    2025     131    1414   1471 -0.926457  0.000000 -0.225257 -0.715262 -0.948658  0.093750  0.401821  1.522440 -0.147218 -0.867938 -1.533011 -0.513380  0.214114 -0.280261  0.848504  0.296495 -1.388265
    240517  2025_131_1471_1414    2025     131    1471   1414  0.926457  0.000000 -0.225257  0.408989  0.490802 -0.172523  1.725430  1.522440 -0.805351 -0.867938 -1.293430  0.661165  2.030509 -0.280261  0.511106 -0.143625 -1.164753
    240518  2025_132_1397_1196    2025     132    1397   1196 -0.595580  0.000000 -0.225257  0.569596 -0.537384 -0.971341  0.401821  1.027637  1.827178  1.541254 -1.293430 -0.709137  0.441164 -0.985276  0.173707  0.736615  0.623343
    240519  2025_132_1397_1196    2025     132    1196   1397  0.595580  0.000000 -0.225257  1.292329  0.285165  0.626295  0.732723  1.192571  1.827178  1.034055  1.102378  0.073893 -0.467034 -1.220281 -0.163691 -1.023865  1.517391
    240520  2025_132_1412_1272    2025     132    1412   1272 -0.794106  0.000000 -0.225257  0.168078 -0.126110  1.691385  0.070919  1.192571  0.510914  0.400057  2.539862 -0.121865 -0.694083  0.424754 -0.501090  0.296495  0.623343
    240521  2025_132_1412_1272    2025     132    1272   1412  0.794106  0.000000 -0.225257  1.131722  1.518987  1.558249 -0.259983 -1.116507  0.017315  0.400057  1.821120  0.661165 -0.012935 -0.280261  1.185903  1.616855 -0.047193
    240522  2025_132_1433_1206    2025     132    1206   1433 -0.330878  0.000000 -0.225257 -0.554655 -1.154295 -0.039387  0.732723  0.202966  0.346381 -0.107141  0.862797 -0.904895 -0.012935 -0.515266  1.523301 -0.143625  0.846855
    240523  2025_132_1433_1206    2025     132    1433   1206  0.330878  0.000000 -0.225257 -0.153137 -1.154295 -1.237613  0.401821  0.862703  1.333579  0.907256  0.144054  0.269650 -0.694083  0.659759  0.173707  2.497094  0.399831
    240524  2025_132_1458_1276    2025     132    1458   1276 -0.397053  0.000000 -0.225257 -1.357692 -1.976844  1.558249  0.070919  3.171781  0.346381 -0.233940  1.102378  1.444195 -1.148182 -1.690291 -1.175887 -0.583745 -0.941241
    240525  2025_132_1458_1276    2025     132    1276   1458  0.397053  0.000000 -0.225257 -0.875870 -1.154295  0.360022  0.732723  2.182176 -0.311751 -0.867938 -1.053849  2.031468  0.214114 -1.220281 -1.175887 -0.583745 -0.494217
    240526  2025_132_1463_1165    2025     132    1463   1165  0.397053  0.000000 -0.225257  1.613544  1.313350  0.759431  2.056332  0.532834  0.181848  0.146458 -0.814269 -0.709137  0.441164 -2.395306 -0.501090 -1.463985 -0.270705
    240527  2025_132_1463_1165    2025     132    1165   1463 -0.397053  0.000000 -0.225257  1.131722  0.902076 -0.571932  1.394528  1.192571  0.181848 -0.487540 -1.533011  0.465408  0.441164 -0.515266 -1.513286 -0.583745 -0.494217
    
    [240528 rows x 22 columns]
    
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 240528 entries, 0 to 240527
    Data columns (total 22 columns):
     #   Column  Non-Null Count   Dtype  
    ---  ------  --------------   -----  
     0   ID      240528 non-null  object 
     1   Season  240528 non-null  int32  
     2   DayNum  240528 non-null  int32  
     3   TeamID  240528 non-null  int32  
     4   OppID   240528 non-null  int32  
     5   Margin  240528 non-null  float32
     6   Loc     240528 non-null  float32
     7   NumOT   240528 non-null  float32
     8   Score   240528 non-null  float32
     9   FGM     240528 non-null  float32
     10  FGA     240528 non-null  float32
     11  FGM3    240528 non-null  float32
     12  FGA3    240528 non-null  float32
     13  FTM     240528 non-null  float32
     14  FTA     240528 non-null  float32
     15  OR      240528 non-null  float32
     16  DR      240528 non-null  float32
     17  Ast     240528 non-null  float32
     18  TO      240528 non-null  float32
     19  Stl     240528 non-null  float32
     20  Blk     240528 non-null  float32
     21  PF      240528 non-null  float32
    dtypes: float32(17), int32(4), object(1)
    memory usage: 21.1+ MB



```python
device = "cuda" if torch.cuda.is_available() else "cpu"
tensor_kwargs = dict(dtype=torch.float32, device=device)
x = torch.tensor(res.loc[:, "Loc":].values, **tensor_kwargs)
print(f"x {x.shape}")
print(x)
```

    x torch.Size([240528, 16])
    tensor([[ 0.0000, -0.2253, -0.1531,  ...,  0.1737, -1.0239,  0.8469],
            [ 0.0000, -0.2253,  0.0075,  ..., -0.8385,  0.2965, -0.0472],
            [ 0.0000, -0.2253, -0.6350,  ...,  0.8485, -0.5837,  0.3998],
            ...,
            [ 0.0000, -0.2253, -0.1531,  ...,  0.1737,  2.4971,  0.3998],
            [ 0.0000, -0.2253, -1.3577,  ..., -1.1759, -0.5837, -0.9412],
            [ 0.0000, -0.2253,  1.6135,  ..., -0.5011, -1.4640, -0.2707]])



```python

```
